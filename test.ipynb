{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare continuous features...\n",
      "Prepare deep features...\n",
      "Prepare wide features...\n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "100000/100000 [==============================] - 42s 425us/step - loss: 6.8334 - val_loss: 1.2032\n",
      "Epoch 2/5\n",
      "100000/100000 [==============================] - 37s 372us/step - loss: 1.0897 - val_loss: 1.0691\n",
      "Epoch 3/5\n",
      "100000/100000 [==============================] - 47s 469us/step - loss: 1.0700 - val_loss: 1.0671\n",
      "Epoch 4/5\n",
      "100000/100000 [==============================] - 59s 591us/step - loss: 1.0603 - val_loss: 1.0616\n",
      "Epoch 5/5\n",
      "100000/100000 [==============================] - 48s 479us/step - loss: 1.0543 - val_loss: 1.0720\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input_4 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input_5 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embed_1 (Embedding)             (None, 1, 64)        91904       Input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Embed_2 (Embedding)             (None, 1, 64)        576         Input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Embed_3 (Embedding)             (None, 1, 64)        320         Input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Embed_4 (Embedding)             (None, 1, 64)        640         Input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Embed_5 (Embedding)             (None, 1, 64)        576         Input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Embed_Flat_1 (Reshape)          (None, 64)           0           Embed_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Embed_Flat_2 (Reshape)          (None, 64)           0           Embed_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Embed_Flat_3 (Reshape)          (None, 64)           0           Embed_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Embed_Flat_4 (Reshape)          (None, 64)           0           Embed_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Embed_Flat_5 (Reshape)          (None, 64)           0           Embed_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Cont_Input (InputLayer)         (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Concat_Embed_Cont (Concatenate) (None, 328)          0           Embed_Flat_1[0][0]               \n",
      "                                                                 Embed_Flat_2[0][0]               \n",
      "                                                                 Embed_Flat_3[0][0]               \n",
      "                                                                 Embed_Flat_4[0][0]               \n",
      "                                                                 Embed_Flat_5[0][0]               \n",
      "                                                                 Cont_Input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Deep_1_Dense (Dense)            (None, 256)          84224       Concat_Embed_Cont[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Deep_1_Acti (LeakyReLU)         (None, 256)          0           Deep_1_Dense[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Deep_1_BN (BatchNormalization)  (None, 256)          1024        Deep_1_Acti[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Deep_2_Dense (Dense)            (None, 128)          32896       Deep_1_BN[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Deep_2_Acti (LeakyReLU)         (None, 128)          0           Deep_2_Dense[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Deep_2_BN (BatchNormalization)  (None, 128)          512         Deep_2_Acti[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Deep_3_Dense (Dense)            (None, 64)           8256        Deep_2_BN[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Deep_3_Acti (LeakyReLU)         (None, 64)           0           Deep_3_Dense[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Deep_3_BN (BatchNormalization)  (None, 64)           256         Deep_3_Acti[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Wide_Input (InputLayer)         (None, 805)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Concat_Deep_Wide (Concatenate)  (None, 869)          0           Deep_3_BN[0][0]                  \n",
      "                                                                 Wide_Input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "FC_1_Dense (Dense)              (None, 128)          111360      Concat_Deep_Wide[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "FC_1_Acti (LeakyReLU)           (None, 128)          0           FC_1_Dense[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "FC_1_BN (BatchNormalization)    (None, 128)          512         FC_1_Acti[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FC_2_Dense (Dense)              (None, 64)           8256        FC_1_BN[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FC_2_Acti (LeakyReLU)           (None, 64)           0           FC_2_Dense[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "FC_2_BN (BatchNormalization)    (None, 64)           256         FC_2_Acti[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FC_3_Dense (Dense)              (None, 32)           2080        FC_2_BN[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FC_3_Acti (LeakyReLU)           (None, 32)           0           FC_3_Dense[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "FC_3_BN (BatchNormalization)    (None, 32)           128         FC_3_Acti[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FC_4_Dense (Dense)              (None, 16)           528         FC_3_BN[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "FC_4_Acti (LeakyReLU)           (None, 16)           0           FC_4_Dense[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "FC_4_BN (BatchNormalization)    (None, 16)           64          FC_4_Acti[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output (Dense)                  (None, 1)            17          FC_4_BN[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 344,385\n",
      "Trainable params: 343,009\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN RMSE:  1.014365499281001\n",
      "VALID RMSE:  1.030363594425346\n",
      "Writing test predictions to file done.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import random\n",
    "from math import sqrt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding\n",
    "from keras.layers import Concatenate, Reshape\n",
    "from keras.layers import LeakyReLU, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "# sess_config = tf.ConfigProto()\n",
    "# sess_config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "# set_session(tf.Session(config=sess_config))\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "random.seed(2019)\n",
    "np.random.seed(2019)\n",
    "tf.set_random_seed(2019)\n",
    "\n",
    "STUDENT_ID = '20420684'\n",
    "\n",
    "embed_size = 64\n",
    "deep_blocks = 3\n",
    "deep_units = 256\n",
    "fc_blocks = 4\n",
    "fc_units = 128\n",
    "lr = 0.00025\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "patience = int(epochs*0.2)\n",
    "\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(pred, actual))\n",
    "\n",
    "\n",
    "def build_deepwide_model(len_continuous, deep_vocab_lens, len_wide, embed_size):\n",
    "    input_list = []\n",
    "    continuous_input = Input(shape=(len_continuous,), dtype='float32', name='Cont_Input')\n",
    "    input_list.append(continuous_input)\n",
    "\n",
    "    emb_list = []\n",
    "    i = 0\n",
    "    for vocab_size in deep_vocab_lens:\n",
    "        i += 1\n",
    "        _input = Input(shape=(1,), dtype='int32', name='Input_'+str(i))\n",
    "        input_list.append(_input)\n",
    "        _emb = Embedding(output_dim=embed_size, input_dim=vocab_size, input_length=1, name='Embed_'+str(i))(_input)\n",
    "        _emb = Reshape((embed_size,), name='Embed_Flat_'+str(i))(_emb)\n",
    "        emb_list.append(_emb)\n",
    "\n",
    "    deep_input = Concatenate(name='Concat_Embed_Cont')(emb_list + [continuous_input])\n",
    "    x = deep_input\n",
    "    # Create blocks\n",
    "    # Dense --> Activation --> BatchNorm\n",
    "    for i in range(deep_blocks):\n",
    "        x = Dense(deep_units//(2**i), name='Deep_'+str(i+1)+'_Dense')(x)\n",
    "        x = LeakyReLU(name='Deep_'+str(i+1)+'_Acti')(x)\n",
    "        x = BatchNormalization(name='Deep_'+str(i+1)+'_BN')(x)\n",
    "\n",
    "    wide_input = Input(shape=(len_wide,), dtype='float32', name='Wide_Input')\n",
    "    input_list.append(wide_input)\n",
    "\n",
    "    # Output Stage\n",
    "    fc_input = Concatenate(name='Concat_Deep_Wide')([x, wide_input])\n",
    "    fc = fc_input\n",
    "    for i in range(fc_blocks):\n",
    "        fc = Dense(fc_units//(2**i), name='FC_'+str(i+1)+'_Dense')(fc)\n",
    "        fc = LeakyReLU(name='FC_'+str(i+1)+'_Acti')(fc)\n",
    "        fc = BatchNormalization(name='FC_'+str(i+1)+'_BN')(fc)\n",
    "\n",
    "    model_output = Dense(1, name='Output')(fc)\n",
    "    model = Model(inputs=input_list, outputs=model_output)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_continuous_features(df, continuous_columns):\n",
    "    continuous_features = df[continuous_columns].values\n",
    "    return continuous_features\n",
    "\n",
    "\n",
    "def get_top_k_p_combinations(df, comb_p, topk, output_freq=False):\n",
    "    def get_category_combinations(categories_str, comb_p=2):\n",
    "        categories = categories_str.split(', ')\n",
    "\n",
    "        return list(combinations(categories, comb_p))\n",
    "\n",
    "    all_categories_p_combos = df[\"item_categories\"].apply(\n",
    "        lambda x: get_category_combinations(x, comb_p)).values.tolist()\n",
    "    all_categories_p_combos = [tuple(t) for item in all_categories_p_combos for t in item]\n",
    "\n",
    "    tmp = dict(Counter(all_categories_p_combos))\n",
    "    sorted_categories_combinations = list(sorted(tmp.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    if output_freq:\n",
    "        return sorted_categories_combinations[:topk]\n",
    "    else:\n",
    "        return [t[0] for t in sorted_categories_combinations[:topk]]\n",
    "\n",
    "\n",
    "def get_wide_features(df):\n",
    "    def categories_to_binary_output(categories):\n",
    "        binary_output = [0 for _ in range(len(selected_categories_to_idx))]\n",
    "        for category in categories.split(', '):\n",
    "            if category in selected_categories_to_idx:\n",
    "                binary_output[selected_categories_to_idx[category]] = 1\n",
    "            else:\n",
    "                binary_output[0] = 1\n",
    "\n",
    "        return binary_output\n",
    "\n",
    "    def categories_cross_transformation(categories):\n",
    "        current_category_set = set(categories.split(', '))\n",
    "        corss_transform_output = [0 for _ in range(len(top_combinations))]\n",
    "        for k, comb_k in enumerate(top_combinations):\n",
    "            if len(current_category_set & comb_k) == len(comb_k):\n",
    "                corss_transform_output[k] = 1\n",
    "            else:\n",
    "                corss_transform_output[k] = 0\n",
    "\n",
    "        return corss_transform_output\n",
    "\n",
    "    category_binary_features = np.array(df.item_categories.apply(\n",
    "        lambda x: categories_to_binary_output(x)).values.tolist())\n",
    "    category_corss_transform_features = np.array(df.item_categories.apply(\n",
    "        lambda x: categories_cross_transformation(x)).values.tolist())\n",
    "\n",
    "    return np.concatenate((category_binary_features, category_corss_transform_features), axis=1)\n",
    "\n",
    "\n",
    "def count_elite(ele):\n",
    "    if ele == '':\n",
    "        return 0\n",
    "    else:\n",
    "        return len(ele.split(','))\n",
    "\n",
    "\n",
    "tr_df = pd.read_csv(\"data/train.csv\")\n",
    "val_df = pd.read_csv(\"data/valid.csv\")\n",
    "te_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "tr_ratings = tr_df.stars.values\n",
    "val_ratings = val_df.stars.values\n",
    "\n",
    "user_df = pd.read_json(\"data/user.json\")\n",
    "item_df = pd.read_json(\"data/business.json\")\n",
    "\n",
    "user_df['elite'] = user_df['elite'].apply(count_elite)\n",
    "\n",
    "attrs_list = pd.Series(['Alcohol', 'Caters', 'NoiseLevel', 'WiFi'])\n",
    "attr_cols = attrs_list.str.lower()\n",
    "\n",
    "for idx in range(item_df.shape[0]):\n",
    "    try:\n",
    "        attr = item_df.loc[idx, 'attributes']\n",
    "    except TypeError:\n",
    "        attr = \"None\"\n",
    "\n",
    "    for i in range(attrs_list.shape[0]):\n",
    "        attr_name = attrs_list[i]\n",
    "        attr_col = attr_cols[i]\n",
    "        if attr != \"None\":\n",
    "            try:\n",
    "                value = attr[attr_name]\n",
    "            except KeyError:\n",
    "                value = \"Missing\"\n",
    "            except TypeError:\n",
    "                value = \"Missing\"\n",
    "\n",
    "            item_df.at[idx, attr_col] = value\n",
    "        else:\n",
    "            item_df.at[idx, attr_col] = attr\n",
    "\n",
    "user_df = user_df.rename(index=str, columns={t: 'user_' + t for t in user_df.columns if t != 'user_id'})\n",
    "item_df = item_df.rename(index=str, columns={t: 'item_' + t for t in item_df.columns if t != 'business_id'})\n",
    "\n",
    "tr_df[\"index\"] = tr_df.index\n",
    "val_df[\"index\"] = val_df.index\n",
    "te_df[\"index\"] = te_df.index\n",
    "\n",
    "tr_df = pd.merge(pd.merge(tr_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n",
    "val_df = pd.merge(pd.merge(val_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n",
    "te_df = pd.merge(pd.merge(te_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n",
    "\n",
    "# Continuous features\n",
    "print(\"Prepare continuous features...\")\n",
    "continuous_columns = [\"user_average_stars\", \"user_review_count\", \"user_useful\", \"item_is_open\", \"item_latitude\", \"item_longitude\", \"item_review_count\", \"item_stars\"]\n",
    "tr_continuous_features = get_continuous_features(tr_df, continuous_columns)\n",
    "val_continuous_features = get_continuous_features(val_df, continuous_columns)\n",
    "te_continuous_features = get_continuous_features(te_df, continuous_columns)\n",
    "\n",
    "scaler = StandardScaler().fit(tr_continuous_features)\n",
    "tr_continuous_features = scaler.transform(tr_continuous_features)\n",
    "val_continuous_features = scaler.transform(val_continuous_features)\n",
    "te_continuous_features = scaler.transform(te_continuous_features)\n",
    "\n",
    "# Deep features\n",
    "print(\"Prepare deep features...\")\n",
    "item_deep_columns = [\"item_postal_code\", \"item_alcohol\", \"item_caters\", \"item_noiselevel\", \"item_wifi\"]\n",
    "item_deep_vocab_lens = []\n",
    "for col_name in item_deep_columns:\n",
    "    tmp = item_df[col_name].unique()\n",
    "    vocab = dict(zip(tmp, range(1, len(tmp) + 1)))\n",
    "    item_deep_vocab_lens.append(len(vocab) + 1)\n",
    "    item_df[col_name + \"_idx\"] = item_df[col_name].apply(lambda x: vocab[x] if x in vocab else 0)\n",
    "item_deep_idx_columns = [t + \"_idx\" for t in item_deep_columns]\n",
    "item_to_deep_features = dict(zip(item_df.business_id.values, item_df[item_deep_idx_columns].values.tolist()))\n",
    "\n",
    "tr_deep_features = np.array(tr_df.business_id.apply(lambda x: item_to_deep_features[x]).values.tolist())\n",
    "val_deep_features = np.array(val_df.business_id.apply(lambda x: item_to_deep_features[x]).values.tolist())\n",
    "te_deep_features = np.array(te_df.business_id.apply(lambda x: item_to_deep_features[x]).values.tolist())\n",
    "\n",
    "# Wide (Category) features\n",
    "print(\"Prepare wide features...\")\n",
    "\n",
    "# Prepare binary encoding for each selected categories\n",
    "all_categories = [category for category_list in item_df.item_categories.values for category in category_list.split(\", \")]\n",
    "category_sorted = sorted(Counter(all_categories).items(), key=lambda x: x[1], reverse=True)\n",
    "selected_categories = [t[0] for t in category_sorted]\n",
    "selected_categories_to_idx = dict(zip(selected_categories, range(1, len(selected_categories) + 1)))\n",
    "selected_categories_to_idx['unk'] = 0\n",
    "idx_to_selected_categories = {val: key for key, val in selected_categories_to_idx.items()}\n",
    "\n",
    "# Prepare Cross transformation for each categories\n",
    "top_combinations = []\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 2, 16, output_freq=False)\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 3, 8, output_freq=False)\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 4, 4, output_freq=False)\n",
    "top_combinations = [set(t) for t in top_combinations]\n",
    "\n",
    "tr_wide_features = get_wide_features(tr_df)\n",
    "val_wide_features = get_wide_features(val_df)\n",
    "te_wide_features = get_wide_features(te_df)\n",
    "\n",
    "# Build input\n",
    "tr_features = []\n",
    "tr_features.append(tr_continuous_features.tolist())\n",
    "tr_features += [tr_deep_features[:, i].tolist() for i in range(len(tr_deep_features[0]))]\n",
    "tr_features.append(tr_wide_features.tolist())\n",
    "\n",
    "val_features = []\n",
    "val_features.append(val_continuous_features.tolist())\n",
    "val_features += [val_deep_features[:, i].tolist() for i in range(len(val_deep_features[0]))]\n",
    "val_features.append(val_wide_features.tolist())\n",
    "\n",
    "te_features = []\n",
    "te_features.append(te_continuous_features.tolist())\n",
    "te_features += [te_deep_features[:, i].tolist() for i in range(len(te_deep_features[0]))]\n",
    "te_features.append(te_wide_features.tolist())\n",
    "\n",
    "# Callbacks\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=patience)\n",
    "mc = ModelCheckpoint('build/best_model.h5', save_best_only=True)\n",
    "cb_list = [es, mc]\n",
    "\n",
    "# Model training\n",
    "deepwide_model = build_deepwide_model(len(tr_continuous_features[0]), item_deep_vocab_lens, len(tr_wide_features[0]), embed_size=embed_size)\n",
    "\n",
    "deepwide_model.compile(optimizer=Adam(lr=lr), loss='mse')\n",
    "\n",
    "history = deepwide_model.fit(tr_features, tr_ratings, epochs=epochs, batch_size=batch_size, validation_data=(val_features, val_ratings), callbacks=cb_list)\n",
    "\n",
    "with open('build/log/baseline_history.json', 'w') as fp:\n",
    "    json.dump(history.history, fp)\n",
    "\n",
    "# Evaluate model\n",
    "print(deepwide_model.summary())\n",
    "\n",
    "best_model = load_model('build/best_model.h5')\n",
    "y_pred = best_model.predict(tr_features)\n",
    "print(\"TRAIN RMSE: \", rmse(y_pred, tr_ratings))\n",
    "y_pred = best_model.predict(val_features)\n",
    "print(\"VALID RMSE: \", rmse(y_pred, val_ratings))\n",
    "\n",
    "# Make Prediction\n",
    "y_pred = best_model.predict(te_features)\n",
    "res_df = pd.DataFrame()\n",
    "res_df['pred'] = y_pred[:, 0]\n",
    "res_df.to_csv(\"build/{}.csv\".format(STUDENT_ID), index=False)\n",
    "print(\"Writing test predictions to file done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
